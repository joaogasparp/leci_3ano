{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpMyt2FCDbfP"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import re\n",
        "import random\n",
        "import shutil\n",
        "import json\n",
        "from json import JSONEncoder\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import callbacks\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import balanced_accuracy_score, f1_score, precision_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from skimage.util import random_noise\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "\n",
        "from numpy import expand_dims\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import save_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import array_to_img\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOnvB-M27SEt"
      },
      "source": [
        "def split_train_test(base_dir, train_dir, test_dir, train_perc, test_perc, class_names, extensions):\n",
        "\n",
        "  if not os.path.exists(train_dir) or not os.path.exists(test_dir):\n",
        "\n",
        "    # Create \"train\" and \"test\" folders\n",
        "    os.mkdir(train_dir)\n",
        "    os.mkdir(test_dir)\n",
        "\n",
        "    for class_name in class_names:\n",
        "      # get all image files beginning with the class name\n",
        "      regex = r'^' + class_name + r'[0-9]+[_0-9]*\\..*'\n",
        "\n",
        "      #print(regex)\n",
        "\n",
        "      filename_list = [fn for fn in os.listdir(base_dir) if re.match(regex, fn) and any(fn.endswith(ext) for ext in extensions)]\n",
        "      #print(filename_list)\n",
        "\n",
        "      # Randomly choose the files for the test set (remaing for training set)\n",
        "      random.seed(15)\n",
        "      n_files = len(filename_list)\n",
        "      test_files = random.sample(filename_list, int(round(float(n_files)*test_perc/100,0)))\n",
        "      train_files = list(set(filename_list) - set(test_files))\n",
        "\n",
        "      # Place the files in the folder with the associated class name within\n",
        "      # folders \"train\" and \"test\", according to the previous selection\n",
        "      train_class_dir = os.path.join(train_dir, class_name)\n",
        "      #print(train_class_dir)\n",
        "      test_class_dir = os.path.join(test_dir, class_name)\n",
        "      #print(test_class_dir)\n",
        "      if not os.path.exists(train_class_dir):\n",
        "        os.mkdir(train_class_dir)\n",
        "      if not os.path.exists(test_class_dir):\n",
        "        os.mkdir(test_class_dir)\n",
        "\n",
        "      for file in train_files:\n",
        "        shutil.move(os.path.join(base_dir, file), os.path.join(train_class_dir, file))\n",
        "      for file in test_files:\n",
        "        shutil.move(os.path.join(base_dir, file), os.path.join(test_class_dir, file))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-Sbl9vJ4YlT"
      },
      "source": [
        "def transfer_images_between_folders(source, dest, filenames, class_labels, class_names):\n",
        "  for filename, label in zip(filenames, class_labels):\n",
        "      shutil.move(os.path.join(source, class_names[label], filename),\n",
        "                  os.path.join(dest, class_names[label], filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE9iSrZ08xVG"
      },
      "source": [
        "def transfer_all_images_between_folders(source, dest, split_rate, class_names):\n",
        "  for class_name in class_names:\n",
        "      transfer_between_folders(source + '/' + class_name, dest + '/' + class_name,\n",
        "                              split_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaiUMFysfmfY"
      },
      "source": [
        "def remove_all_images(folder, class_names):\n",
        "  for class_name in class_names:\n",
        "    for f in os.listdir(os.path.join(folder, class_name)):\n",
        "      os.remove(os.path.join(folder, class_name, f))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1HLTtHz4bv3"
      },
      "source": [
        "def transfer_between_folders(source, dest, split_rate):\n",
        "\n",
        "  source_files = os.listdir(source)\n",
        "  if(len(source_files) != 0):\n",
        "      transfer_file_nr = int(len(source_files)*split_rate)\n",
        "      transfer_ind = random.sample(range(0, len(source_files)), transfer_file_nr)\n",
        "      for ind in transfer_ind:\n",
        "          shutil.move(os.path.join(source, str(source_files[ind])),\n",
        "                      os.path.join(dest, str(source_files[ind])))\n",
        "  # else:\n",
        "      # print(\"No file moved. Source empty!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0mcVLKsC_h4"
      },
      "source": [
        "def get_X_y(base_dir, class_labels):\n",
        "\n",
        "  X=[]\n",
        "  y=[]\n",
        "\n",
        "  for i in range(len(class_labels)):\n",
        "    label = class_labels[i]\n",
        "    files = os.listdir(os.path.join(base_dir, label))\n",
        "    for filename in files:\n",
        "      X.append(filename)\n",
        "      y.append(i)\n",
        "\n",
        "  X=np.asarray(X)\n",
        "  y=np.asarray(y)\n",
        "\n",
        "  return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hReqIzD-Hb2E"
      },
      "source": [
        "def get_X_y_groups(base_dir, class_labels):\n",
        "\n",
        "  X = []\n",
        "  y = []\n",
        "  groups = []\n",
        "\n",
        "  for i in range(len(class_labels)):\n",
        "      label = class_labels[i]\n",
        "      files = os.listdir(os.path.join(base_dir, label))\n",
        "      for filename in files:\n",
        "          vals = filename.split(\".\")[0].split(\"_\")\n",
        "          id = vals[1] + \"_\" + vals[2]\n",
        "\n",
        "          X.append(filename)\n",
        "          y.append(i)\n",
        "          groups.append(id)\n",
        "\n",
        "  X = np.asarray(X)\n",
        "  y = np.asarray(y)\n",
        "  groups = np.asarray(groups)\n",
        "\n",
        "  return X, y, groups"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldxNMyX26GXC"
      },
      "source": [
        "def KFoldStratificationClassGroup(n_folds, groups, X, y):\n",
        "\n",
        "  unique_groups = list(set(groups))  # get lists with subjects and gestures\n",
        "  unique_classes = list(set(y))\n",
        "\n",
        "  test_indexes = list()  # create lists to append splits\n",
        "  train_indexes = list()\n",
        "\n",
        "  # print(y)\n",
        "  # print(groups)\n",
        "\n",
        "  for class_ in unique_classes:  # iterate over gestures\n",
        "    for group in unique_groups:  # iterate over subjects\n",
        "\n",
        "      indexesToSplit = np.array([i for i in range(len(groups)) if y[i] == class_ and groups[i] == group]) #get indexes of gestures to split\n",
        "\n",
        "      # print(indexesToSplit)\n",
        "\n",
        "      kf = KFold(n_splits=n_folds, shuffle=True, random_state=15)\n",
        "      n_fold=0\n",
        "      for train_index, test_index in kf.split(X[indexesToSplit], y[indexesToSplit], groups[indexesToSplit]):\n",
        "\n",
        "        # print(train_index)\n",
        "        # print(test_index)\n",
        "\n",
        "        if len(train_indexes) == n_folds and len(test_indexes) == n_folds:\n",
        "          train_indexes[n_fold] = np.append(train_indexes[n_fold], indexesToSplit[train_index])\n",
        "          test_indexes[n_fold] = np.append(test_indexes[n_fold], indexesToSplit[test_index])\n",
        "        else:\n",
        "          train_indexes.append(indexesToSplit[train_index])\n",
        "          test_indexes.append(indexesToSplit[test_index])\n",
        "\n",
        "        # print(train_indexes)\n",
        "        # print(test_indexes)\n",
        "\n",
        "        n_fold = n_fold + 1\n",
        "\n",
        "  # print(train_indexes)\n",
        "  # print(test_indexes)\n",
        "\n",
        "  return train_indexes, test_indexes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7twvFRTWAQPg"
      },
      "source": [
        "def print_metrics(y_true, y_pred, class_names, model_name):\n",
        "\n",
        "  accuracy = balanced_accuracy_score(y_true, y_pred)\n",
        "  precision = precision_score(y_true, y_pred, average='weighted')\n",
        "  f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "  print(f\"Balanced Accuracy (%)  : {accuracy*100:.2f}\")\n",
        "  #print(f\"Weighted Precision (%) : {precision*100:.2f}\")\n",
        "  #print(f\"Weighted F1 score (%)  : {f1*100:.2f}\")\n",
        "\n",
        "  print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "  cm = confusion_matrix(y_true, y_pred, normalize = \"true\")\n",
        "  # print(cm)\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(5, 4.5), tight_layout=True)\n",
        "  ax.set_title(f\"Normalized Confusion Matrix - {model_name}\", fontweight=\"bold\", pad=20, fontsize=14)\n",
        "  sns.heatmap(cm, annot=True, cmap=plt.cm.Blues)\n",
        "  ax.set_xticklabels(class_names)\n",
        "  ax.set_yticklabels(class_names)\n",
        "  plt.ylabel('True label', fontweight=\"bold\")\n",
        "  plt.xlabel('Predicted label', fontweight=\"bold\")\n",
        "  # Rotate the tick labels and set their alignment.\n",
        "  plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "          rotation_mode=\"anchor\")\n",
        "  plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\",\n",
        "          rotation_mode=\"anchor\")\n",
        "  plt.show()\n",
        "\n",
        "  #return accuracy, precision, f1Score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS4AYQkpyfdt"
      },
      "source": [
        "def get_metrics(y_true, y_pred, class_names, groups, model_name):\n",
        "\n",
        "  # Get metrics considering all groups\n",
        "\n",
        "  # Balanced accuracy (not given by classification_report)\n",
        "  balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
        "  # precision = precision_score(y_true, y_pred, average='weighted')\n",
        "  # f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "  # Classification report (including several metrics)\n",
        "  #print(y_true)\n",
        "  #print(y_pred)\n",
        "  report = classification_report(y_true, y_pred, target_names=class_names, output_dict = True)\n",
        "  #print(report)\n",
        "\n",
        "  # Confusion matrix\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "  # Get list of unique groups\n",
        "  unique_groups = list(set(groups))\n",
        "\n",
        "  report_groups = None\n",
        "  balanced_acc_groups = None\n",
        "  cm_groups = None\n",
        "\n",
        "  # If there are 2 groups or more\n",
        "  if len(unique_groups) > 1:\n",
        "\n",
        "    report_groups = list()\n",
        "    balanced_acc_groups = list()\n",
        "    cm_groups = list()\n",
        "\n",
        "    # Additionally get the same metrics for each group separately\n",
        "    for group in unique_groups:\n",
        "\n",
        "      ind = [i for i in range(len(groups)) if groups[i]==group]\n",
        "      y_true_group = np.array(y_true)[ind]\n",
        "      y_pred_group = np.array(y_pred)[ind]\n",
        "\n",
        "      balanced_acc_group = balanced_accuracy_score(y_true_group, y_pred_group)\n",
        "      report_group = classification_report(y_true_group, y_pred_group, target_names=class_names, output_dict = True)\n",
        "      #print(report_group)\n",
        "      cm_group = confusion_matrix(y_true_group, y_pred_group)\n",
        "\n",
        "      report_groups.append(report_group)\n",
        "      balanced_acc_groups.append(balanced_acc_group)\n",
        "      cm_groups.append(cm_group)\n",
        "\n",
        "  return report, balanced_acc, cm, report_groups, balanced_acc_groups, cm_groups"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8nlbnn9YoYr"
      },
      "source": [
        "def plot_history(inc_history, base_model_name):\n",
        "\n",
        "  # Visualize training history\n",
        "  fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, sharex=True)\n",
        "\n",
        "  # summarize history for accuracy\n",
        "  if 'categorical_accuracy' in inc_history.history:\n",
        "    ax0.plot(inc_history.history['categorical_accuracy'])\n",
        "  if 'val_categorical_accuracy' in inc_history.history:\n",
        "    ax0.plot(inc_history.history['val_categorical_accuracy'])\n",
        "  ax0.set_title('Model Accuracy')\n",
        "  ax0.set_ylabel('Accuracy')\n",
        "  ax0.set_xlabel('No. epoch')\n",
        "  ax0.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "  # summarize history for loss\n",
        "  if 'loss' in inc_history.history:\n",
        "    ax1.plot(inc_history.history['loss'])\n",
        "  if 'val_loss' in inc_history.history:\n",
        "    ax1.plot(inc_history.history['val_loss'])\n",
        "  ax1.set_title('Model Loss')\n",
        "  ax1.set_ylabel('Loss')\n",
        "  ax1.set_xlabel('No. epoch')\n",
        "  ax1.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "  fig.suptitle(base_model_name)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sudJzu2QO3De"
      },
      "source": [
        "def write_to_csv(dir, results_fold, dataset_name, class_names, n_images_fold, base_model_name, model_params, aug_params, outer_fold_nr, groupsDict, testType):\n",
        "\n",
        "  csv_dir = os.path.join(dir, \"Results\")\n",
        "\n",
        "  print(csv_dir)\n",
        "\n",
        "  if not os.path.exists(csv_dir):\n",
        "    os.mkdir(csv_dir)\n",
        "\n",
        "  #now = datetime.now()\n",
        "  #d1 = now.strftime(\"%d-%m-%Y_%H:%M:%S\")\n",
        "\n",
        "  folder_name = \"Neurons \" + str(model_params[\"nr_hidden_neurons\"]) + \"_dropout \" + str(model_params[\"dropout_rate\"]) + \"_min delta \" + str(model_params[\"min_delta\"]) + \"_patience \" + str(model_params[\"patience\"]) + \"_optimizer \" + str(model_params[\"optimizer\"])\n",
        "  config_dir = os.path.join(csv_dir,folder_name)\n",
        "\n",
        "  if not os.path.exists(config_dir):\n",
        "    os.mkdir(config_dir)\n",
        "\n",
        "  final_dir = config_dir\n",
        "  if aug_params:\n",
        "    aug_dir_name = \"No_augmentation\"\n",
        "    if aug_params[\"augment_all\"]:\n",
        "      aug_dir_name = \"Augmented all \" + str(aug_params[\"n_images_all\"])\n",
        "    if aug_params[\"augment_all\"] and aug_params[\"augment_train\"]:\n",
        "      aug_dir_name += \"_\"\n",
        "      if aug_params[\"augment_train\"]:\n",
        "        aug_dir_name += \"Augmented train \" + str(aug_params[\"n_images_all\"])\n",
        "    elif aug_params[\"augment_train\"]:\n",
        "        aug_dir_name = \"Augmented train \" + str(aug_params[\"n_images_all\"])\n",
        "    aug_dir = os.path.join(config_dir, aug_dir_name)\n",
        "    if not os.path.exists(aug_dir):\n",
        "      os.mkdir(aug_dir)\n",
        "    final_dir = aug_dir\n",
        "\n",
        "  file_path = os.path.join(final_dir, dataset_name)\n",
        "  print(file_path)\n",
        "  filename1 = file_path + \".csv\"\n",
        "  filename2 = file_path + \"_Classific_Report.csv\"\n",
        "  filename3 = file_path + \"_Conf_Matrix.json\"\n",
        "\n",
        "  exists1 = os.path.exists(filename1)\n",
        "  exists2 = os.path.exists(filename2)\n",
        "  exists3 = os.path.exists(filename3)\n",
        "\n",
        "  # CSV files\n",
        "  with open(filename1, 'a') as writefile1, open(filename2, 'a') as writefile2:\n",
        "\n",
        "    # Write headers\n",
        "    if not exists1:\n",
        "      header1 = \"Test_Type,n_subj_test,FullAugmentation,FullAugFactor,TrainAugFactor,TrainAugmentation,pre_trained_model,outer_fold,inner_fold,epoch,loss,categorical_acc,val_loss,val_categorical_acc,epoch_time (s),test_time (s),\" +\\\n",
        "                \"TrainSubjects,TestSubjects\\n\"\n",
        "      writefile1.write(header1)\n",
        "\n",
        "    if not exists2:\n",
        "      header2 = \"Test_Type,n_subj_test,FullAugmentation,FullAugFactor,TrainAugFactor,TrainAugmentation,pre_trained_model,subject,outer_fold,inner_fold,epochs,\"\n",
        "      for class_name in class_names:\n",
        "        header2 += class_name + \"_precision,\" + class_name + \"_recall,\" + class_name + \"_f1,\" + class_name + \"_support,\"\n",
        "      header2 += \"macro_avg_precision,macro_avg_recall,macro_avg_f1,macro_avg_support,\" +\\\n",
        "        \"weighted_avg_precision,weighted_avg_recall,weighted_avg_f1,weighted_avg_support,\" +\\\n",
        "        \"accuracy,balanced_accuracy,\" +\\\n",
        "        \"train_time (s),predict_time (s),n_train_images,n_val_images,n_test_images,TrainSubjects,TestSubjects\\n\"\n",
        "      writefile2.write(header2)\n",
        "\n",
        "    if aug_params[\"augment_all\"] == True and aug_params[\"augment_train\"] == True:\n",
        "      toAdd = str(aug_params[\"augment_all\"]) + \",\" + str(aug_params[\"n_images_all\"]) + \",\" + str(aug_params[\"augment_train\"]) + \",\" + str(aug_params[\"n_images_train\"])\n",
        "    elif aug_params[\"augment_all\"] == True and aug_params[\"augment_train\"] == False:\n",
        "      toAdd = str(aug_params[\"augment_all\"]) + \",\" + str(aug_params[\"n_images_all\"]) + \",\" + str(aug_params[\"augment_train\"]) + \",\" + \"None\"\n",
        "    elif aug_params[\"augment_all\"] == False and aug_params[\"augment_train\"] == True:\n",
        "      toAdd = str(aug_params[\"augment_all\"]) + \",\" + \"None\" + \",\"  + str(aug_params[\"augment_train\"]) + \",\" + str(aug_params[\"n_images_train\"])\n",
        "    else:\n",
        "      toAdd = str(aug_params[\"augment_all\"]) + \",\" + \"None\" + \",\"  + str(aug_params[\"augment_train\"]) + \",\" + \"None\"\n",
        "\n",
        "    # Write results\n",
        "    # File 2 (folds)\n",
        "    data_str = get_report_results_str(results_fold[\"classif_report\"], results_fold[\"balanced_acc\"], results_fold[\"inc_history\"], results_fold[\"times\"], \\\n",
        "                                      n_images_fold, class_names, \"All\", base_model_name, aug_params, toAdd, outer_fold_nr, groupsDict, testType)\n",
        "    writefile2.write(data_str)\n",
        "\n",
        "    if (\"groups\" in results_fold) and results_fold[\"groups\"] and (len(results_fold[\"groups\"]) > 0) and (len(results_fold[\"groups\"][0]) > 1):\n",
        "      subj_ind = 0\n",
        "\n",
        "      groups = results_fold[\"groups\"][0]\n",
        "\n",
        "      for subj_ind in range(len(groups)):\n",
        "        subj_name = groups[subj_ind]\n",
        "        print(groups[subj_ind])\n",
        "\n",
        "        classific_report = list()\n",
        "        balanced_acc = list()\n",
        "\n",
        "        for classific_report_group, balanced_acc_group in zip(results_fold[\"classif_report_groups\"], results_fold[\"balanced_acc_groups\"]):\n",
        "          classific_report.append(classific_report_group[subj_ind])\n",
        "          balanced_acc.append(balanced_acc_group[subj_ind])\n",
        "\n",
        "        data_str = get_report_results_str(classific_report, balanced_acc, None, None, None, class_names, subj_name, base_model_name, aug_params, toAdd, outer_fold_nr, groupsDict, testType)\n",
        "        writefile2.write(data_str)\n",
        "\n",
        "    # File 1 (epochs)\n",
        "    fold_nr = 0\n",
        "    for fold in results_fold[\"inc_history\"]:\n",
        "      # trainSubject, testSubject = groupsDict[fold_nr].replace(\"'\",\"\").replace(\"}\",\"\").replace(\"{\",\"\").split(\"/\")\n",
        "      for epoch in range(len(fold.history[\"loss\"])):\n",
        "        data_str = testType + \",\"\n",
        "        if groupsDict is not None:\n",
        "          data_str += str(len(groupsDict[fold_nr][\"test\"]))\n",
        "        else:\n",
        "          data_str += str(0)\n",
        "        data_str += \",\" + toAdd + \",\" + base_model_name +\",\"+ str((outer_fold_nr)) +\",\"+ str(fold_nr+1) + \",\" + str(epoch + 1) + \",\" +\\\n",
        "           str(fold.history[\"loss\"][epoch]) + \",\" + str(fold.history[\"categorical_accuracy\"][epoch]) + \",\" +\\\n",
        "           str(fold.history[\"val_loss\"][epoch]) + \",\" + str(fold.history[\"val_categorical_accuracy\"][epoch]) + \",\" +\\\n",
        "           str(results_fold[\"times\"][fold_nr].epoch_times[epoch]) + \",\" + str(results_fold[\"times\"][fold_nr].test_times[epoch])\n",
        "        if groupsDict is not None:\n",
        "          data_str += \",\" + \";\".join(groupsDict[fold_nr][\"train\"]) + \",\" + \";\".join(groupsDict[fold_nr][\"test\"]) + \"\\n\"\n",
        "        else:\n",
        "          data_str += \",,\\n\"\n",
        "        writefile1.write(data_str)\n",
        "      fold_nr += 1\n",
        "\n",
        "  # JSON file\n",
        "  json_data = {\"cm_results\": []}\n",
        "  if exists3:\n",
        "    with open(filename3, \"r\") as readfile3:\n",
        "      json_data = json.load(readfile3)\n",
        "\n",
        "  cm_results = {\"pre_trained_model\": base_model_name, \"labels\": class_names, \"cm_info\": []}\n",
        "\n",
        "  cm_info_all = {\"subject\": \"All\", \"folds\": []}\n",
        "  for cm in results_fold[\"cm\"]:\n",
        "    cm_info_all[\"folds\"].append(cm.tolist())\n",
        "  cm_results[\"cm_info\"].append(cm_info_all)\n",
        "\n",
        "  if (\"groups\" in results_fold) and results_fold[\"groups\"] and (len(results_fold[\"groups\"]) > 0) and (len(results_fold[\"groups\"][0]) > 1):\n",
        "    group_ind = 0\n",
        "    for group in results_fold[\"groups\"][0]:\n",
        "      cm_info_group = {\"subject\": group, \"folds\": []}\n",
        "      for cm_groups_fold in results_fold[\"cm_groups\"]:\n",
        "        cm_info_group[\"folds\"].append(cm_groups_fold[group_ind].tolist())\n",
        "\n",
        "      cm_results[\"cm_info\"].append(cm_info_group)\n",
        "      group_ind += 1\n",
        "\n",
        "  json_data[\"cm_results\"].append(cm_results)\n",
        "\n",
        "  with open(filename3, \"w\") as writefile3:\n",
        "    json.dump(json_data, writefile3)\n",
        "\n",
        "  #print(inc_history_fold.history)\n",
        "  #print(classif_report_fold)\n",
        "  #print(balanced_acc_fold)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx9B1LxmV7Yr"
      },
      "source": [
        "def get_report_results_str(report_folds, balanced_acc_folds, inc_history_fold, times_fold, n_images_fold, class_names, group_name, base_model_name, aug_params, toAdd, outer_fold_nr, groupsDict, testType):\n",
        "\n",
        "  data_str = \"\"\n",
        "\n",
        "  # print(report_folds)\n",
        "  # print(class_names)\n",
        "\n",
        "  fold_nr = 0\n",
        "  for report in report_folds:\n",
        "    # trainSubject, testSubject = groupsDict[fold_nr].replace(\"'\",\"\").replace(\"}\",\"\").replace(\"{\",\"\").split(\"/\")\n",
        "\n",
        "    data_str += testType + \",\"\n",
        "    if groupsDict is not None:\n",
        "       data_str += str(len(groupsDict[fold_nr][\"test\"]))\n",
        "    else:\n",
        "       data_str += str(0)\n",
        "    data_str += \",\" + toAdd + \",\" + base_model_name + \",\" + group_name\n",
        "\n",
        "    if outer_fold_nr is not None:\n",
        "      data_str += \",\" + str(outer_fold_nr)\n",
        "\n",
        "    data_str += \",\" + str(fold_nr+1) + \",\"\n",
        "\n",
        "    if inc_history_fold is not None:\n",
        "       data_str += str(len(inc_history_fold[fold_nr].history['loss']))\n",
        "\n",
        "    for class_name in class_names:\n",
        "      data_str += \",\" + str(report[class_name][\"precision\"]) + \",\" + str(report[class_name][\"recall\"]) + \",\" + str(report[class_name][\"f1-score\"]) + \",\" + str(report[class_name][\"support\"])\n",
        "\n",
        "    data_str += \",\" + str(report[\"macro avg\"][\"precision\"]) + \",\" + str(report[\"macro avg\"][\"recall\"]) + \",\" + str(report[\"macro avg\"][\"f1-score\"]) + \",\" + str(report[\"macro avg\"][\"support\"]) + \",\" +\\\n",
        "        str(report[\"weighted avg\"][\"precision\"]) + \",\" + str(report[\"weighted avg\"][\"recall\"]) + \",\" + str(report[\"weighted avg\"][\"f1-score\"]) + \",\" + str(report[\"weighted avg\"][\"support\"]) + \",\" +\\\n",
        "        str(report[\"accuracy\"]) + \",\" + str(balanced_acc_folds[fold_nr])\n",
        "\n",
        "    if times_fold is not None:\n",
        "        data_str += \",\" + str(times_fold[fold_nr].train_times[0]) + \",\" + str(times_fold[fold_nr].predict_times[0])\n",
        "    else:\n",
        "        data_str += \",,\"\n",
        "\n",
        "    if n_images_fold is not None:\n",
        "      if aug_params is not None and \"augment_train\" in aug_params and aug_params[\"augment_train\"] is True:\n",
        "        data_str += \",\" + str(n_images_fold[\"train_aug\"][fold_nr])\n",
        "      else:\n",
        "        data_str += \",\" + str(n_images_fold[\"train\"][fold_nr])\n",
        "\n",
        "      data_str += \",\" + str(n_images_fold[\"val\"][fold_nr]) + \",\" + str(n_images_fold[\"test\"][fold_nr]) + \",\"\n",
        "      if groupsDict is not None:\n",
        "        data_str += \";\".join(groupsDict[fold_nr][\"train\"]) + \",\" + \";\".join(groupsDict[fold_nr][\"test\"]) + \"\\n\"\n",
        "      else:\n",
        "        data_str += \",\\n\"\n",
        "    else:\n",
        "      data_str += \",,,,\\n\"\n",
        "\n",
        "    fold_nr += 1\n",
        "\n",
        "  return data_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN7GHxn3W0ey"
      },
      "source": [
        "class TimeHistory(keras.callbacks.Callback):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.epoch_times = []\n",
        "    self.train_times = []\n",
        "    self.test_times = []\n",
        "    self.predict_times = []\n",
        "\n",
        "  def on_epoch_begin(self, batch, logs={}):\n",
        "    self.epoch_time_start = time.time()\n",
        "\n",
        "  def on_epoch_end(self, batch, logs={}):\n",
        "    self.epoch_times.append(time.time() - self.epoch_time_start)\n",
        "\n",
        "  def on_train_begin(self, logs={}):\n",
        "    self.train_time_start = time.time()\n",
        "\n",
        "  def on_train_end(self, batch, logs={}):\n",
        "    self.train_times.append(time.time() - self.train_time_start)\n",
        "\n",
        "  def on_test_begin(self, logs={}):\n",
        "    self.test_time_start = time.time()\n",
        "\n",
        "  def on_test_end(self, batch, logs={}):\n",
        "    self.test_times.append(time.time() - self.test_time_start)\n",
        "\n",
        "  def on_predict_begin(self, logs={}):\n",
        "    self.predict_time_start = time.time()\n",
        "\n",
        "  def on_predict_end(self, batch, logs={}):\n",
        "    self.predict_times.append(time.time() - self.predict_time_start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFylodRGjSQ2"
      },
      "source": [
        "def write_host_specs(host_specs, dataset_name, model_params, aug_params):\n",
        "\n",
        "  dir = \"/content/drive/MyDrive/\"\n",
        "  csv_dir = os.path.join(dir, \"csv\")\n",
        "\n",
        "  folder_name = \"Neurons \" + str(model_params[\"nr_hidden_neurons\"]) + \"_dropout \" + str(model_params[\"dropout_rate\"]) + \"_min delta \" + str(model_params[\"min_delta\"]) + \"_patience \" + str(model_params[\"patience\"]) + \"_optimizer \" + str(model_params[\"optimizer\"])\n",
        "  config_dir = os.path.join(csv_dir,folder_name)\n",
        "\n",
        "  if not os.path.exists(config_dir):\n",
        "    os.mkdir(config_dir)\n",
        "\n",
        "  final_dir = config_dir\n",
        "  if aug_params:\n",
        "    aug_dir_name = \"No_augmentation\"\n",
        "    if aug_params[\"augment_all\"]:\n",
        "      aug_dir_name = \"Augmented all \" + str(aug_params[\"n_images_all\"])\n",
        "    if aug_params[\"augment_all\"] and aug_params[\"augment_train\"]:\n",
        "      aug_dir_name += \"_\"\n",
        "      if aug_params[\"augment_train\"]:\n",
        "        aug_dir_name += \"Augmented train \" + str(aug_params[\"n_images_all\"])\n",
        "    elif aug_params[\"augment_train\"]:\n",
        "        aug_dir_name = \"Augmented train \" + str(aug_params[\"n_images_all\"])\n",
        "    aug_dir = os.path.join(config_dir, aug_dir_name)\n",
        "    if not os.path.exists(aug_dir):\n",
        "      os.mkdir(aug_dir)\n",
        "    final_dir = aug_dir\n",
        "\n",
        "  file_path = os.path.join(final_dir, dataset_name + \"_Host_Specs\")\n",
        "  filename1 = file_path + \".txt\"\n",
        "  with open(filename1, 'a') as writefile1:\n",
        "    for key in host_specs.keys():\n",
        "      writefile1.write(key + \": \")\n",
        "      writefile1.write(host_specs[key] + \"\\n\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkC_pG1ski5W"
      },
      "source": [
        "def augment_data_classic_noises(source, dest, class_names, n_new):\n",
        "\n",
        "  # print(\"Augmentation\")\n",
        "\n",
        "  noise_types = [\"gaussian\", \"salt\", \"pepper\", \"poisson\"]\n",
        "\n",
        "  count = 0\n",
        "\n",
        "  # For each image belonging to each class in source folder\n",
        "  for class_name in class_names:\n",
        "\n",
        "    # print(class_name)\n",
        "    # print(len(os.listdir(os.path.join(dest, class_name))))\n",
        "\n",
        "    files = os.listdir(os.path.join(source, class_name))\n",
        "    # print(len(files))\n",
        "\n",
        "    for filename in files:\n",
        "\n",
        "      # print(filename)\n",
        "\n",
        "      # Load the image\n",
        "      file_path = os.path.join(source, class_name, filename)\n",
        "      img = cv2.imread(file_path)\n",
        "      # img = Image.open(file_path)\n",
        "\n",
        "      # Add noise to the image \"n_new\" times\n",
        "      for i in range(n_new):\n",
        "        noise_img = img\n",
        "\n",
        "        # Choose randomly nr of times to add noise (between 1 and 3 times)\n",
        "        #n_layers = random.randint(1, len(noise_types))\n",
        "        n_layers = 3\n",
        "\n",
        "        # For each type of noise\n",
        "        for j in range(n_layers):\n",
        "\n",
        "          # Decide which noise type to add\n",
        "          noise = random.choice(noise_types)\n",
        "\n",
        "          # Add the type of noise to the image\n",
        "          if noise == \"salt\" or noise == \"pepper\":\n",
        "            amount = random.uniform(0.001,0.003)\n",
        "            noise_img_array = random_noise(noise_img, mode=noise, amount=amount) # Returns a floating-point image on the range [0, 1]\n",
        "          elif noise == \"gaussian\":\n",
        "            var = random.uniform(0.008,0.01)\n",
        "            noise_img_array = random_noise(noise_img, mode=noise, var=var) # Returns a floating-point image on the range [0, 1]\n",
        "          else:\n",
        "            noise_img_array = random_noise(noise_img, mode=noise) # Returns a floating-point image on the range [0, 1]\n",
        "\n",
        "          # noise_img = Image.fromarray(noise_img_array)\n",
        "          noise_img = np.array(noise_img, dtype = 'uint8')\n",
        "\n",
        "        # Save new image with noise to the corresponding class folder in the destination folder\n",
        "        p = Path(filename)\n",
        "        new_filename = str(p.stem) + \"_aug_\" + str(i+1) + str(p.suffix)\n",
        "        new_file_path = os.path.join(dest, class_name, new_filename)\n",
        "        cv2.imwrite(new_file_path, noise_img)\n",
        "        # noise_img.save(os.path.joint(dest, class_name, filename), noise_img)\n",
        "\n",
        "    count += len(os.listdir(os.path.join(dest, class_name)))\n",
        "    # print(count)\n",
        "\n",
        "  return count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0BbdwZK6E0W"
      },
      "source": [
        "def augment_data_shift(source, dest, class_names, n_new):\n",
        "\n",
        "  count = 0\n",
        "\n",
        "  # For each image belonging to each class in source folder\n",
        "  for class_name in class_names:\n",
        "\n",
        "    # print(class_name)\n",
        "    # print(len(os.listdir(os.path.join(dest, class_name))))\n",
        "\n",
        "    files = os.listdir(os.path.join(source, class_name))\n",
        "    # print(len(files))\n",
        "\n",
        "    for filename in files:\n",
        "\n",
        "      # print(filename)\n",
        "\n",
        "      # Load the image\n",
        "      file_path = os.path.join(source, class_name, filename)\n",
        "      # print(file_path)\n",
        "      img = load_img(file_path)\n",
        "\n",
        "      # convert to numpy array\n",
        "      data = img_to_array(img)\n",
        "      # expand dimension to one sample\n",
        "      samples = expand_dims(data, 0)\n",
        "\n",
        "      # create image data augmentation generator\n",
        "      datagen = ImageDataGenerator(width_shift_range=[-0.5,0.5], fill_mode = \"constant\", cval = 0)\n",
        "\n",
        "      # prepare iterator\n",
        "      it = datagen.flow(samples, batch_size=1)\n",
        "\n",
        "      # Add noise to the image \"n_new\" times\n",
        "      for i in range(n_new):\n",
        "        # noise_img = img\n",
        "\n",
        "        # generate batch of images\n",
        "        batch = it.next()\n",
        "        augmented_img_array = batch[0]\n",
        "\n",
        "        augmented_img = array_to_img(augmented_img_array)\n",
        "\n",
        "        # Save augmented image to the corresponding class folder in the destination folder\n",
        "        p = Path(filename)\n",
        "        new_filename = str(p.stem) + \"_aug_\" + str(i+1) + str(p.suffix)\n",
        "        new_file_path = os.path.join(dest, class_name, new_filename)\n",
        "        # cv2.imwrite(os.path.join(dest, class_name, new_filename), noise_img)\n",
        "        # noise_img.save(os.path.joint(dest, class_name, filename), noise_img)\n",
        "        save_img(new_file_path, augmented_img)\n",
        "\n",
        "        # print(new_file_path)\n",
        "\n",
        "        if False:\n",
        "          drive_dir = \"/content/drive/MyDrive/aug_images\"\n",
        "          if not os.path.exists(drive_dir):\n",
        "            os.mkdir(drive_dir)\n",
        "          for class_name_ in class_names:\n",
        "            if not os.path.exists(os.path.join(drive_dir, class_name_)):\n",
        "              os.mkdir(os.path.join(drive_dir, class_name_))\n",
        "          print(os.path.join(drive_dir, class_name, new_filename))\n",
        "          save_img(os.path.join(drive_dir, class_name, new_filename), augmented_img)\n",
        "\n",
        "    count += len(os.listdir(os.path.join(dest, class_name)))\n",
        "    # print(count)\n",
        "\n",
        "  return count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyNH1ITJx8Qg"
      },
      "source": [
        "def augment_data_noise_blocks(source, dest, class_names, n_new):\n",
        "  count = 0\n",
        "\n",
        "  # For each image belonging to each class in source folder\n",
        "  for class_name in class_names:\n",
        "    # print(class_name)\n",
        "    # print(len(os.listdir(os.path.join(dest, class_name))))\n",
        "\n",
        "    files = os.listdir(os.path.join(source, class_name))\n",
        "    # print(len(files))\n",
        "    for filename in files:\n",
        "      # print(filename)\n",
        "\n",
        "      # Load the image\n",
        "      file_path = os.path.join(source, class_name, filename)\n",
        "\n",
        "      # print(file_path)\n",
        "      img = Image.open(file_path)\n",
        "      img = np.array(img)\n",
        "\n",
        "      for i in range(n_new):\n",
        "        img_to_add = np.zeros(img.shape) #create empty matrix with image size\n",
        "\n",
        "        height, width, depth = img.shape\n",
        "\n",
        "        #print(\"Size: \" + str(img.size))\n",
        "        size = (155,155)\t#size to convert the images to, 155X155 is the input size for the classifier in use on 13/06/2021\n",
        "\n",
        "        # print(\"========== Copy \" + str(i) + \" ===========\")\n",
        "        nrOfBlocks = random.randrange(1,6)\t#if number of blocks wasnt passed to function, generates it\n",
        "\n",
        "        for j in range(nrOfBlocks):\n",
        "\n",
        "          blkSize = (random.randrange(1, round((height/8) + 1)), random.randrange(1, round((width/8) + 1)))  #generate random block size\n",
        "\n",
        "          if blkSize[1]%2 == 0:\t#deals with even or odd blksizes\n",
        "            gradientImageHalf1 = np.linspace(1,255,(int(blkSize[1]/2)))\t#creates evenly distributed interval of numbers between 1 and 255\n",
        "            gradientImageHalf2 = np.flipud(gradientImageHalf1) #creates evenly distributed interval of numbers between 255 and 1\n",
        "          else:\n",
        "            gradientImageHalf1 = np.linspace(1,255,(int(blkSize[1]/2 + 1))) #creates evenly distributed interval of numbers between 1 and 255\n",
        "            gradientImageHalf2 = np.linspace(255,1,(int(blkSize[1]/2))) #creates evenly distributed interval of numbers between 255 and 1\n",
        "\n",
        "          #print(gradientImageHalf1.size)\n",
        "          #print(gradientImageHalf2.size)\n",
        "          gradientImage = np.append(gradientImageHalf1,gradientImageHalf2,axis = 0) #appends both intervals\n",
        "          gradientImage = np.tile(gradientImage, (blkSize[0],1)) #tiles the first row creating a matrix with blksize\n",
        "          #print(gradientImage)\n",
        "          rows,columns = gradientImage.shape #get matrix dimensions\n",
        "\n",
        "          #lowers the values of the edge rows creating a gradient in the block to mimic real noise\n",
        "          gradientImage[0] = gradientImage[0] / 4\n",
        "          #gradientImage[1] = gradientImage[1] / 3\n",
        "          #gradientImage[2] = gradientImage[2] / 2\n",
        "          gradientImage[rows-1] = gradientImage[rows-1] / 4\n",
        "          #gradientImage[rows-2] = gradientImage[rows-2] / 3\n",
        "          #gradientImage[rows-3] = gradientImage[rows-3] / 2\n",
        "\n",
        "          gradientImage[:, 0] = gradientImage[:, 0] / 4\n",
        "          #gradientImage[:, 1] = gradientImage[:, 1] / 3\n",
        "          #gradientImage[:, 2] = gradientImage[:, 2] / 2\n",
        "          gradientImage[:, columns-1] = gradientImage[:, columns-1] / 4\n",
        "          #gradientImage[:, columns-2] = gradientImage[:, columns-2] / 3\n",
        "          #gradientImage[:, columns-3] = gradientImage[:, columns-3] / 2\n",
        "\n",
        "          # print(gradientImage)\n",
        "          # x = input()\n",
        "\n",
        "\n",
        "          #blkCenterRow = random.randrange(0 + blkSize[0], img.size[0] - blkSize[0])\n",
        "          blkCenterRow = random.randrange(0 + blkSize[0], img.shape[0] - blkSize[0])\n",
        "          #blkCenterColumn = random.randrange(0 + blkSize[1], img.size[1]- blkSize[1])\n",
        "          blkCenterColumn = random.randrange(0 + blkSize[1], img.shape[1]- blkSize[1])\n",
        "\n",
        "          intensityRandomFactor = random.random() #create random intesity factor that will be used if intensity == none\n",
        "          for row in range(blkCenterRow - int(blkSize[0]/2),blkCenterRow + int(blkSize[0]/2)): #iterates from blkCenterRow - int(blkSize[0]/2) to blkCenterRow + int(blkSize[0]/2)\n",
        "            for column in range(blkCenterColumn - int(blkSize[1]/2),blkCenterColumn + int(blkSize[1]/2)):\n",
        "              #if no intensity was passed, get intensity value from gradient image and multiply by intensity factor\n",
        "              img_to_add[row][column] = gradientImage[np.remainder(row,(blkCenterRow - int(blkSize[0]/2)))][np.remainder(column,(blkCenterColumn - int(blkSize[1]/2)))] * intensityRandomFactor\n",
        "\n",
        "        #print(img.size)\n",
        "        #print(img_to_add.shape)\n",
        "\n",
        "        #img = img_to_array(img)\n",
        "        #noise_img = Image.fromarray(img + np.transpose(img_to_add)) #create image\n",
        "\n",
        "        noise_img = array_to_img(img + np.array(img_to_add))\n",
        "\n",
        "        #noise_img = noise_img.convert(\"RGB\")\n",
        "        #noise_img = noise_img.resize(size)\n",
        "        #noise_img.show()\n",
        "        #x = input()\n",
        "\n",
        "        p = Path(filename)\n",
        "        new_filename = str(p.stem) + \"_aug_\" + str(i+1) + str(p.suffix)\n",
        "        new_file_path = os.path.join(dest, class_name, new_filename)\n",
        "\n",
        "        save_img(new_file_path, noise_img)\n",
        "\n",
        "        # print(new_filename)\n",
        "\n",
        "    count += len(os.listdir(os.path.join(dest, class_name)))\n",
        "    # print(count)\n",
        "\n",
        "  return count"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}